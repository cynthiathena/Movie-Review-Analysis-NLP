{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Milestone - Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR_ygfqF3G5l"
      },
      "source": [
        "#Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzM5_TLn3GOU"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from copy import deepcopy\n",
        "import regex as re\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aavaI-TJ28fi"
      },
      "source": [
        "# Load CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10VPOAHdSaLH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EPHtCARW4aV"
      },
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNSdfgPf235T"
      },
      "source": [
        "# ga bisa:(\n",
        "# df = pd.read_csv('/content/drive/Shareddrives/PowerPuffGirls/NLP/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "hVUVpDjVsX0Z",
        "outputId": "9ea920ce-27ab-4833-af65-494fa72e9018"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-806afa0e-2957-4426-b565-c23210c04998\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-806afa0e-2957-4426-b565-c23210c04998\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving IMDB Dataset.csv to IMDB Dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGcnSiZDbZmA"
      },
      "source": [
        "df = pd.read_csv('IMDB Dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "VQqpwpGW9rrj",
        "outputId": "b76f4f5e-1133-4729-8d3e-ba29dc8f607e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afovz1It91Wk",
        "outputId": "f636aff0-4b65-4c5f-cd0f-ff307c70f0cd"
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    25000\n",
              "positive    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VojRp1u-epU",
        "outputId": "64ffd767-2644-40f5-d989-49e99bb146a3"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APHcyCLM7B3F"
      },
      "source": [
        "df_original = deepcopy(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "EAoTd62i9yxj",
        "outputId": "4bfd0024-0089-42d2-bac0-1e2fc5e10b3b"
      },
      "source": [
        "df_original.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6AIWlMD2MJD"
      },
      "source": [
        "# Spelling Correction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1neDlQCMbsiR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "a30b70a2-b775-4ff2-b0fe-c49c3ae74ffe"
      },
      "source": [
        "df['review'][1304]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"This is one of the best movies. It is one of my favorites. A movie with good acting. The story is very sensitive and touching. Good camera work also.<br /><br />The names of the actresses and actors are not at the top of the American Star list. However, they give equal or better performances than the top of the list.<br /><br />It is such a pleasure to see a movie about true love, romance, friendship without having to endure watching someone having to kick-box their way to save the world.<br /><br />If you don't like this movie then you have no heart or feelings. Then go watch a sports movie. There is no killing or horror here. See the movie. It is a must. TH\""
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "IjWVpTwv6DEa",
        "outputId": "b1fe40bd-fb4b-4122-f138-ce2237d48dc3"
      },
      "source": [
        "def remove_tags(string):\n",
        "    result = re.sub(\"<.*?()>|[^a-zA-Z1-9.']|[.]+\",' ',string)\n",
        "    return result\n",
        "\n",
        "df['review_split'] = df['review'].apply(lambda x: remove_tags(x.lower()).split())\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review_split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[one, of, the, other, reviewers, has, mentione...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[a, wonderful, little, production, the, filmin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[i, thought, this, was, a, wonderful, way, to,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[basically, there's, a, family, where, a, litt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[petter, mattei's, love, in, the, time, of, mo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ...                                       review_split\n",
              "0  One of the other reviewers has mentioned that ...  ...  [one, of, the, other, reviewers, has, mentione...\n",
              "1  A wonderful little production. <br /><br />The...  ...  [a, wonderful, little, production, the, filmin...\n",
              "2  I thought this was a wonderful way to spend ti...  ...  [i, thought, this, was, a, wonderful, way, to,...\n",
              "3  Basically there's a family where a little boy ...  ...  [basically, there's, a, family, where, a, litt...\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  ...  [petter, mattei's, love, in, the, time, of, mo...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oBkyC9A5s8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b239bf40-d1d0-489a-f5ce-3a9dcbfcf8be"
      },
      "source": [
        "reviews = []\n",
        "for i in range (df.shape[0]):\n",
        "  reviews += df['review_split'][i]\n",
        "\n",
        "print(len(reviews))\n",
        "# reviews[1304]\n",
        "\n",
        "unique = np.unique(np.array(reviews))\n",
        "\n",
        "print(len(unique))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11561340\n",
            "120922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SmD4_X4jyYW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "85ceb007-1d31-4b03-bd7b-a131d335442c"
      },
      "source": [
        "unique[10000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ambushing'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBtnawZsZt94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230b248e-3b25-45d2-8a42-ba7aa72fab7e"
      },
      "source": [
        "# # characters -> int\n",
        "# vocab_dict = {}\n",
        "# count = 0\n",
        "# for word in unique:\n",
        "#   for character in word:\n",
        "#       if character not in vocab_dict:\n",
        "#           vocab_dict[character] = count\n",
        "#           count += 1\n",
        "\n",
        "# # Add special tokens\n",
        "# codes = ['<PAD>','<EOS>','<GO>']\n",
        "# for code in codes:\n",
        "#     vocab_dict[code] = count\n",
        "#     count += 1\n",
        "\n",
        "# sorted(vocab_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'\",\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " '<EOS>',\n",
              " '<GO>',\n",
              " '<PAD>',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDANrqelEXds"
      },
      "source": [
        "# # Create another dictionary to convert integers to their respective characters\n",
        "# int_to_vocab = {}\n",
        "# for character, value in vocab_dict.items():\n",
        "#     int_to_vocab[value] = character"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3esc5E_D0fd"
      },
      "source": [
        "# # Convert sentences to integers\n",
        "# int_sentences = []\n",
        "\n",
        "# for sentence in reviews:\n",
        "#     int_sentence = []\n",
        "#     for word in sentence:\n",
        "#       int_words = []\n",
        "#       for character in word:\n",
        "#         int_words.append(vocab_dict[character])\n",
        "#       int_sentence.append(int_words)\n",
        "#     int_sentences.append(int_sentence)\n",
        "\n",
        "\n",
        "# print(int_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y19rbqaN14YR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f98d0b86-8322-4988-d0d2-67de95ecd4cc"
      },
      "source": [
        "# len(reviews[0][0])\n",
        "\n",
        "# reviews[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'o'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Clz_9odKnSb"
      },
      "source": [
        "# # Method to Relocate, remove, or add characters to create spelling mistakes\n",
        "# letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
        "#            'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
        "\n",
        "# def noise_maker(sentence, threshold):   \n",
        "    \n",
        "#     noisy_sentence = []\n",
        "#     i = 0\n",
        "#     while i < len(sentence):\n",
        "#         random = np.random.uniform(0,1,1)\n",
        "#         # Most characters will be correct since the threshold value is high\n",
        "#         if random < threshold:\n",
        "#             noisy_sentence.append(sentence[i])\n",
        "#         else:\n",
        "#             new_random = np.random.uniform(0,1,1)\n",
        "#             # ~33% chance characters will swap locations\n",
        "#             if new_random > 0.67:\n",
        "#                 if i == (len(sentence) - 1):\n",
        "#                     # If last character in sentence, it will not be typed\n",
        "#                     continue\n",
        "#                 else:\n",
        "#                     # if any other character, swap order with following character\n",
        "#                     noisy_sentence.append(sentence[i+1])\n",
        "#                     noisy_sentence.append(sentence[i])\n",
        "#                     i += 1\n",
        "#             # ~33% chance an extra lower case letter will be added to the sentence\n",
        "#             elif new_random < 0.33:\n",
        "#                 random_letter = np.random.choice(letters, 1)[0]\n",
        "#                 noisy_sentence.append(vocab_to_int[random_letter])\n",
        "#                 noisy_sentence.append(sentence[i])\n",
        "#             # ~33% chance a character will not be typed\n",
        "#             else:\n",
        "#                 pass     \n",
        "#         i += 1\n",
        "#     return noisy_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRhMlum525vU"
      },
      "source": [
        "# import random\n",
        "# #thresh - 0 to 1\n",
        "# def gen_gibberish(line,thresh=0.2):\n",
        "#     times = int(random.randrange(1,len(line)) * thresh)\n",
        "#     '''\n",
        "#     Types of replacement:\n",
        "#         1.Delete random character.\n",
        "#         2.Add random character.\n",
        "#         3.Replace a character.\n",
        "#         4.Combination?\n",
        "#     '''\n",
        "#     while times!=0:\n",
        "#         # try to gen noise length times...\n",
        "#         times-=1\n",
        "#         val = random.randrange(0,10)\n",
        "#         if val <= 5:\n",
        "#             #get random index\n",
        "#             val = random.randrange(0,10)\n",
        "#             index = random.randrange(2,len(line))\n",
        "#             if val <= 3 :\n",
        "#                 #delete character\n",
        "#                 line = line[:index]+line[index+1:]\n",
        "#             else:\n",
        "#                 #add character\n",
        "#                 insert_index = random.randrange(0,len(char_set))\n",
        "#                 line = line[:index] + char_set[insert_index] + line[index:]\n",
        "#         else:\n",
        "#             index = random.randrange(0,len(char_set))\n",
        "#             replace_index = random.randrange(2,len(line))\n",
        "#             line = line[:replace_index] + char_set[index] + line[replace_index+1:]\n",
        "#     return line\n",
        "\n",
        "# sample = reviews[5][0]\n",
        "# gib = gen_gibberish(sample)\n",
        "\n",
        "\n",
        "# gib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yJv4YZ-2WTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac037e0-4c3b-4eda-f157-5638f6b1601e"
      },
      "source": [
        "# def edits1(word):\n",
        "#     \"All edits that are one edit away from `word`.\"\n",
        "#     letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "#     splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "#     deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "#     transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "#     replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "#     inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "#     return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "\n",
        "# print(len(edits1(\"i\")))\n",
        "\n",
        "# input_texts = []\n",
        "# target_texts = []\n",
        "# # REPEAT_FACTOR = 10\n",
        "\n",
        "# for word in unique[:1000]:\n",
        "#   if len(word)>1:\n",
        "#     output_text = word\n",
        "#     input_text = edits1(word)\n",
        "#     for inp in input_text: \n",
        "#       target_texts.append(word)\n",
        "#       input_texts.append(inp)\n",
        "\n",
        "# print(len(output_text))\n",
        "# print(len(input_text))\n",
        "\n",
        "\n",
        "# # lines = reviews[0]\n",
        "# # SKIP = int(len(lines)*0.65)\n",
        "\n",
        "\n",
        "# # for line in lines[SKIP:]:\n",
        "# #     if len(line)>1:\n",
        "# #         output_text = '\\t' + line + '\\n'\n",
        "# #         for _ in range(REPEAT_FACTOR):\n",
        "# #             input_text = get_gibberish(line)\n",
        "# #             input_texts.append(input_text)\n",
        "# #             target_texts.append(output_text)\n",
        "# # print(\"LEN OF SAMPLES:\",len(input_texts))\n",
        "\n",
        "# # input_texts\n",
        "# # print(input_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n",
            "10\n",
            "550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMG98xuBshID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f979d99-8ffa-4a9c-8a51-03d925831f45"
      },
      "source": [
        "# print(len(target_texts))\n",
        "# print(len(input_texts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "418434\n",
            "418434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX7cRJiaOjcq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "180c7aea-ac8a-42f7-8043-c32b7b1ead42"
      },
      "source": [
        "output_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"'cheaters'\""
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG-fXUHLo3QC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d87a5ea9-2b9c-4e46-f722-a38951ecc672"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input,LSTM,Dense\n",
        "\n",
        "\n",
        "char_set = list(\" abcdefghijklmnopqrstuvwxyz0123456789\")\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1000\n",
        "latent_dim = 256\n",
        "\n",
        "num_enc_tokens = len(char_set)\n",
        "num_dec_tokens = len(char_set) + 2 # includes \\n \\t\n",
        "encoder_inputs = Input(shape=(None,num_enc_tokens))\n",
        "encoder = LSTM(latent_dim,return_state=True)\n",
        "encoder_outputs , state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h,state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,num_dec_tokens))\n",
        "decoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
        "decoder_ouputs,_,_ = decoder_lstm(decoder_inputs,initial_state = encoder_states)\n",
        "\n",
        "decoder_dense = Dense(num_dec_tokens, activation='softmax')\n",
        "decoder_ouputs = decoder_dense(decoder_ouputs)\n",
        "\n",
        "model = Model([encoder_inputs,decoder_inputs],decoder_ouputs)\n",
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 37)]   0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 39)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 256),        301056      ['input_1[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  303104      ['input_2[0][0]',                \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 39)     10023       ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 614,183\n",
            "Trainable params: 614,183\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swBn-m2qs1PX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7b638d-637a-426e-9d19-b1ca78c18d1c"
      },
      "source": [
        "max_enc_len = max([len(x) for x in input_texts])\n",
        "max_dec_len = max([len(x) for x in target_texts])\n",
        "print(\"Max Enc Len:\",max_enc_len)\n",
        "print(\"Max Dec Len:\",max_dec_len)\n",
        "\n",
        "\n",
        "#filling in the enc,dec datas\n",
        "for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n",
        "    for t,char in enumerate(input_text):\n",
        "        encoder_input_data[ i , t , char2int[char] ] = 1\n",
        "    for t,char in enumerate(target_text):\n",
        "        decoder_input_data[ i, t , char2int[char] ] = 1\n",
        "        if t > 0 :\n",
        "            decoder_target_data[ i , t-1 , char2int[char] ] = 1\n",
        "print(\"COMPLETED...\")    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Enc Len: 28\n",
            "Max Dec Len: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQhmuUlstIc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78542a7f-988b-4551-dcff-d8b06e8c25d9"
      },
      "source": [
        "num_samples = len(input_texts)\n",
        "encoder_input_data = np.zeros( (num_samples , max_enc_len , len(char_set)),dtype='float32' )\n",
        "decoder_input_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\n",
        "decoder_target_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\n",
        "print(\"CREATED ZERO VECTORS\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREATED ZERO VECTORS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd1rSvMgLNf5"
      },
      "source": [
        "# h=model.fit([encoder_input_data,decoder_input_data],decoder_target_data\n",
        "#          ,epochs = epochs,\n",
        "#           batch_size = batch_size,\n",
        "#           validation_split = 0.2\n",
        "#          )\n",
        "# model.save('s2s.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO8epFJ60zBU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UU8Fed1Lc1l"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): \n",
        "  result = []\n",
        "  for sentence in text:\n",
        "    for word in sentence:\n",
        "      result += re.findall(r'\\w+', word)\n",
        "  return result\n",
        "\n",
        "WORDS = Counter(words(reviews))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def known(words): \n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def candidates(word): \n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def edits1(word):\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3Whzxr9Ls1J"
      },
      "source": [
        "spelling_correction = [] \n",
        "for i in unique : \n",
        "  for j in i : \n",
        "    spelling_correction.append(correction(j))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxCYSz20Bnem"
      },
      "source": [
        "def correct_array(arr):\n",
        "  res = []\n",
        "  for word in arr:\n",
        "    res.append(correction(word))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAXIg5_p2RPk"
      },
      "source": [
        "# Sentiment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LtC2es94JWV"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTGRFlPGAVUw"
      },
      "source": [
        "df['sentiment_binary'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wabTvHDJ4m5"
      },
      "source": [
        "## Remove tags and split into array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB9HyIRPJR4G"
      },
      "source": [
        "def remove_tags(string):\n",
        "    result = re.sub(\"<.*?>|[^a-zA-Z1-9.']\",' ',string)\n",
        "    return result\n",
        "\n",
        "df['review_split'] = df['review'].apply(lambda x: remove_tags(x.lower()).split())\n",
        "df['review_split'] = df['review_split'].apply(lambda x: correct_array(x))\n",
        "\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBTKNZMd5yuO"
      },
      "source": [
        "## Stop words removal "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWyRezuU_EoT"
      },
      "source": [
        "# ! python -m nltk.downloader stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO7aNbtD51Fq",
        "outputId": "945148f3-0d54-4897-9925-c595b5a0f6e1"
      },
      "source": [
        "def stop_words_removal(text) : \n",
        "  stops = set(stopwords.words(\"english\"))\n",
        "  return [word for word in text if word not in (stops)]\n",
        "\n",
        "df['review_wo_stop_words'] = df['review_split'].apply(stop_words_removal)\n",
        "# test = df['review'].apply(stop_words_removal)\n",
        "print(df['review_split'][0])\n",
        "print(df['review_wo_stop_words'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'n', 'e', ' ', 'f', ' ', 'h', 'e', ' ', 'h', 'e', 'r', ' ', 'r', 'e', 'v', 'e', 'w', 'e', 'r', ' ', 'h', ' ', 'e', 'n', 'n', 'e', ' ', 'h', ' ', 'f', 'e', 'r', ' ', 'w', 'c', 'h', 'n', 'g', ' ', 'j', 'u', ' ', '1', ' ', 'O', 'z', ' ', 'e', 'p', 'e', ' ', 'u', \"'\", 'l', 'l', ' ', 'b', 'e', ' ', 'h', 'k', 'e', '.', ' ', 'T', 'h', 'e', ' ', 'r', 'e', ' ', 'r', 'g', 'h', ',', ' ', ' ', 'h', ' ', ' ', 'e', 'x', 'c', 'l', ' ', 'w', 'h', ' ', 'h', 'p', 'p', 'e', 'n', 'e', ' ', 'w', 'h', ' ', 'e', '.', '<', 'b', 'r', ' ', '/', '>', '<', 'b', 'r', ' ', '/', '>', 'T', 'h', 'e', ' ', 'f', 'r', ' ', 'h', 'n', 'g', ' ', 'h', ' ', 'r', 'u', 'c', 'k', ' ', 'e', ' ', 'b', 'u', ' ', 'O', 'z', ' ', 'w', ' ', ' ', 'b', 'r', 'u', 'l', ' ', 'n', ' ', 'u', 'n', 'f', 'l', 'n', 'c', 'h', 'n', 'g', ' ', 'c', 'e', 'n', 'e', ' ', 'f', ' ', 'v', 'l', 'e', 'n', 'c', 'e', ',', ' ', 'w', 'h', 'c', 'h', ' ', 'e', ' ', 'n', ' ', 'r', 'g', 'h', ' ', 'f', 'r', ' ', 'h', 'e', ' ', 'w', 'r', ' ', 'G', 'O', '.', ' ', 'T', 'r', 'u', ' ', 'e', ',', ' ', 'h', ' ', ' ', 'n', ' ', ' ', 'h', 'w', ' ', 'f', 'r', ' ', 'h', 'e', ' ', 'f', 'n', ' ', 'h', 'e', 'r', 'e', ' ', 'r', ' ', '.', ' ', 'T', 'h', ' ', 'h', 'w', ' ', 'p', 'u', 'l', 'l', ' ', 'n', ' ', 'p', 'u', 'n', 'c', 'h', 'e', ' ', 'w', 'h', ' ', 'r', 'e', 'g', 'r', ' ', ' ', 'r', 'u', 'g', ',', ' ', 'e', 'x', ' ', 'r', ' ', 'v', 'l', 'e', 'n', 'c', 'e', '.', ' ', 'I', ' ', ' ', 'h', 'r', 'c', 'r', 'e', ',', ' ', 'n', ' ', 'h', 'e', ' ', 'c', 'l', 'c', ' ', 'u', 'e', ' ', 'f', ' ', 'h', 'e', ' ', 'w', 'r', '.', '<', 'b', 'r', ' ', '/', '>', '<', 'b', 'r', ' ', '/', '>', 'I', ' ', ' ', 'c', 'l', 'l', 'e', ' ', 'O', 'Z', ' ', ' ', 'h', ' ', ' ', 'h', 'e', ' ', 'n', 'c', 'k', 'n', 'e', ' ', 'g', 'v', 'e', 'n', ' ', ' ', 'h', 'e', ' ', 'O', 'w', 'l', ' ', 'M', 'x', 'u', ' ', 'S', 'e', 'c', 'u', 'r', ' ', 'S', 'e', ' ', 'P', 'e', 'n', 'e', 'n', 'r', '.', ' ', 'I', ' ', 'f', 'c', 'u', 'e', ' ', 'n', 'l', ' ', 'n', ' ', 'E', 'e', 'r', 'l', ' ', 'C', ',', ' ', 'n', ' ', 'e', 'x', 'p', 'e', 'r', 'e', 'n', 'l', ' ', 'e', 'c', 'n', ' ', 'f', ' ', 'h', 'e', ' ', 'p', 'r', 'n', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 'l', 'l', ' ', 'h', 'e', ' ', 'c', 'e', 'l', 'l', ' ', 'h', 'v', 'e', ' ', 'g', 'l', ' ', 'f', 'r', 'n', ' ', 'n', ' ', 'f', 'c', 'e', ' ', 'n', 'w', 'r', ',', ' ', ' ', 'p', 'r', 'v', 'c', ' ', ' ', 'n', ' ', 'h', 'g', 'h', ' ', 'n', ' ', 'h', 'e', ' ', 'g', 'e', 'n', '.', ' ', 'E', ' ', 'C', ' ', ' ', 'h', 'e', ' ', ' ', 'n', '.', '.', 'A', 'r', 'n', ',', ' ', 'M', 'u', 'l', ',', ' ', 'g', 'n', 'g', ',', ' ', 'L', 'n', ',', ' ', 'C', 'h', 'r', 'n', ',', ' ', 'I', 'l', 'n', ',', ' ', 'I', 'r', 'h', ' ', 'n', ' ', 'r', 'e', '.', '.', '.', '.', ' ', 'c', 'u', 'f', 'f', 'l', 'e', ',', ' ', 'e', 'h', ' ', 'r', 'e', ',', ' ', 'g', ' ', 'e', 'l', 'n', 'g', ' ', 'n', ' ', 'h', ' ', 'g', 'r', 'e', 'e', 'e', 'n', ' ', 'r', 'e', ' ', 'n', 'e', 'v', 'e', 'r', ' ', 'f', 'r', ' ', 'w', '.', '<', 'b', 'r', ' ', '/', '>', '<', 'b', 'r', ' ', '/', '>', 'I', ' ', 'w', 'u', 'l', ' ', ' ', 'h', 'e', ' ', 'n', ' ', 'p', 'p', 'e', 'l', ' ', 'f', ' ', 'h', 'e', ' ', 'h', 'w', ' ', ' ', 'u', 'e', ' ', ' ', 'h', 'e', ' ', 'f', 'c', ' ', 'h', ' ', ' ', 'g', 'e', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 'h', 'e', 'r', ' ', 'h', 'w', ' ', 'w', 'u', 'l', 'n', \"'\", ' ', 'r', 'e', '.', ' ', 'F', 'r', 'g', 'e', ' ', 'p', 'r', 'e', ' ', 'p', 'c', 'u', 'r', 'e', ' ', 'p', 'n', 'e', ' ', 'f', 'r', ' ', 'n', 'r', 'e', ' ', 'u', 'e', 'n', 'c', 'e', ',', ' ', 'f', 'r', 'g', 'e', ' ', 'c', 'h', 'r', ',', ' ', 'f', 'r', 'g', 'e', ' ', 'r', 'n', 'c', 'e', '.', '.', '.', 'O', 'Z', ' ', 'e', 'n', \"'\", ' ', 'e', ' ', 'r', 'u', 'n', '.', ' ', 'T', 'h', 'e', ' ', 'f', 'r', ' ', 'e', 'p', 'e', ' ', 'I', ' ', 'e', 'v', 'e', 'r', ' ', 'w', ' ', 'r', 'u', 'c', 'k', ' ', 'e', ' ', ' ', ' ', 'n', ' ', ' ', 'w', ' ', 'u', 'r', 'r', 'e', 'l', ',', ' ', 'I', ' ', 'c', 'u', 'l', 'n', \"'\", ' ', ' ', 'I', ' ', 'w', ' ', 'r', 'e', ' ', 'f', 'r', ' ', ',', ' ', 'b', 'u', ' ', ' ', 'I', ' ', 'w', 'c', 'h', 'e', ' ', 'r', 'e', ',', ' ', 'I', ' ', 'e', 'v', 'e', 'l', 'p', 'e', ' ', ' ', 'e', ' ', 'f', 'r', ' ', 'O', 'z', ',', ' ', 'n', ' ', 'g', ' ', 'c', 'c', 'u', 'e', ' ', ' ', 'h', 'e', ' ', 'h', 'g', 'h', ' ', 'l', 'e', 'v', 'e', 'l', ' ', 'f', ' ', 'g', 'r', 'p', 'h', 'c', ' ', 'v', 'l', 'e', 'n', 'c', 'e', '.', ' ', 'N', ' ', 'j', 'u', ' ', 'v', 'l', 'e', 'n', 'c', 'e', ',', ' ', 'b', 'u', ' ', 'n', 'j', 'u', 'c', 'e', ' ', '(', 'c', 'r', 'k', 'e', ' ', 'g', 'u', 'r', ' ', 'w', 'h', \"'\", 'l', 'l', ' ', 'b', 'e', ' ', 'l', ' ', 'u', ' ', 'f', 'r', ' ', ' ', 'n', 'c', 'k', 'e', 'l', ',', ' ', 'n', 'e', ' ', 'w', 'h', \"'\", 'l', 'l', ' ', 'k', 'l', 'l', ' ', 'n', ' ', 'r', 'e', 'r', ' ', 'n', ' ', 'g', 'e', ' ', 'w', ' ', 'w', 'h', ' ', ',', ' ', 'w', 'e', 'l', 'l', ' ', 'n', 'n', 'e', 'r', 'e', ',', ' ', 'l', 'e', ' ', 'c', 'l', ' ', 'n', 'e', ' ', 'b', 'e', 'n', 'g', ' ', 'u', 'r', 'n', 'e', ' ', 'n', ' ', 'p', 'r', 'n', ' ', 'b', 'c', 'h', 'e', ' ', 'u', 'e', ' ', ' ', 'h', 'e', 'r', ' ', 'l', 'c', 'k', ' ', 'f', ' ', 'r', 'e', 'e', ' ', 'k', 'l', 'l', ' ', 'r', ' ', 'p', 'r', 'n', ' ', 'e', 'x', 'p', 'e', 'r', 'e', 'n', 'c', 'e', ')', ' ', 'W', 'c', 'h', 'n', 'g', ' ', 'O', 'z', ',', ' ', 'u', ' ', ' ', 'b', 'e', 'c', 'e', ' ', 'c', 'f', 'r', 'b', 'l', 'e', ' ', 'w', 'h', ' ', 'w', 'h', ' ', ' ', 'u', 'n', 'c', 'f', 'r', 'b', 'l', 'e', ' ', 'v', 'e', 'w', 'n', 'g', '.', '.', '.', '.', 'h', ' ', 'f', ' ', 'u', ' ', 'c', 'n', ' ', 'g', 'e', ' ', 'n', ' ', 'u', 'c', 'h', ' ', 'w', 'h', ' ', 'u', 'r', ' ', 'r', 'k', 'e', 'r', ' ', 'e', '.']\n",
            "['One', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'Oz', 'episode', \"you'll\", 'be', 'hooked.', 'They', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me.', 'The', 'first', 'thing', 'that', 'struck', 'me', 'about', 'Oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'GO.', 'Trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid.', 'This', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence.', 'Its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word.', 'It', 'is', 'called', 'OZ', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'Oswald', 'Maximum', 'Security', 'State', 'Penitentary.', 'It', 'focuses', 'mainly', 'on', 'Emerald', 'City', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda.', 'Em', 'City', 'is', 'home', 'to', 'many..Aryans', 'Muslims', 'gangstas', 'Latinos', 'Christians', 'Italians', 'Irish', 'and', 'more....so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away.', 'I', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', \"wouldn't\", 'dare.', 'Forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance...OZ', \"doesn't\", 'mess', 'around.', 'The', 'first', 'episode', 'I', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'I', \"couldn't\", 'say', 'I', 'was', 'ready', 'for', 'it', 'but', 'as', 'I', 'watched', 'more', 'I', 'developed', 'a', 'taste', 'for', 'Oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence.', 'Not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', \"who'll\", 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', \"who'll\", 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'Watching', 'Oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing....thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side.']\n",
            "['One', 'reviewers', 'mentioned', 'watching', '1', 'Oz', 'episode', 'hooked.', 'They', 'right', 'exactly', 'happened', 'me.', 'The', 'first', 'thing', 'struck', 'Oz', 'brutality', 'unflinching', 'scenes', 'violence', 'set', 'right', 'word', 'GO.', 'Trust', 'show', 'faint', 'hearted', 'timid.', 'This', 'show', 'pulls', 'punches', 'regards', 'drugs', 'sex', 'violence.', 'Its', 'hardcore', 'classic', 'use', 'word.', 'It', 'called', 'OZ', 'nickname', 'given', 'Oswald', 'Maximum', 'Security', 'State', 'Penitentary.', 'It', 'focuses', 'mainly', 'Emerald', 'City', 'experimental', 'section', 'prison', 'cells', 'glass', 'fronts', 'face', 'inwards', 'privacy', 'high', 'agenda.', 'Em', 'City', 'home', 'many..Aryans', 'Muslims', 'gangstas', 'Latinos', 'Christians', 'Italians', 'Irish', 'more....so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'shady', 'agreements', 'never', 'far', 'away.', 'I', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'goes', 'shows', 'dare.', 'Forget', 'pretty', 'pictures', 'painted', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance...OZ', 'mess', 'around.', 'The', 'first', 'episode', 'I', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'I', 'say', 'I', 'ready', 'I', 'watched', 'I', 'developed', 'taste', 'Oz', 'got', 'accustomed', 'high', 'levels', 'graphic', 'violence.', 'Not', 'violence', 'injustice', 'crooked', 'guards', \"who'll\", 'sold', 'nickel', 'inmates', \"who'll\", 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmates', 'turned', 'prison', 'bitches', 'due', 'lack', 'street', 'skills', 'prison', 'experience', 'Watching', 'Oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewing....thats', 'get', 'touch', 'darker', 'side.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "UEFC2LkmKdi4",
        "outputId": "60346619-2671-4714-80bb-11f3b816b2c3"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>sentiment_binary</th>\n",
              "      <th>review_split</th>\n",
              "      <th>review_wo_stop_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>[One, of, the, other, reviewers, has, mentione...</td>\n",
              "      <td>[One, reviewers, mentioned, watching, 1, Oz, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>[A, wonderful, little, production., The, filmi...</td>\n",
              "      <td>[A, wonderful, little, production., The, filmi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>[I, thought, this, was, a, wonderful, way, to,...</td>\n",
              "      <td>[I, thought, wonderful, way, spend, time, hot,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>[Basically, there's, a, family, where, a, litt...</td>\n",
              "      <td>[Basically, there's, family, little, boy, Jake...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>[Petter, Mattei's, Love, in, the, Time, of, Mo...</td>\n",
              "      <td>[Petter, Mattei's, Love, Time, Money, visually...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ...                               review_wo_stop_words\n",
              "0  One of the other reviewers has mentioned that ...  ...  [One, reviewers, mentioned, watching, 1, Oz, e...\n",
              "1  A wonderful little production. <br /><br />The...  ...  [A, wonderful, little, production., The, filmi...\n",
              "2  I thought this was a wonderful way to spend ti...  ...  [I, thought, wonderful, way, spend, time, hot,...\n",
              "3  Basically there's a family where a little boy ...  ...  [Basically, there's, family, little, boy, Jake...\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  ...  [Petter, Mattei's, Love, Time, Money, visually...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcIvVDEw4Pry"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psAevQJ7Edm4"
      },
      "source": [
        "# ! python -m nltk.downloader wordnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdxGAMty4Iqj",
        "outputId": "d36815f6-175c-4d64-f016-a3c50fcf2075"
      },
      "source": [
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(w) for w in text] \n",
        "\n",
        "df['review_lemmatize'] = df['review_wo_stop_words'].apply(lemmatize_text)\n",
        "print(df['review_wo_stop_words'][0])\n",
        "print(df['review_lemmatize'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'reviewers', 'mentioned', 'watching', '1', 'Oz', 'episode', 'hooked.', 'They', 'right', 'exactly', 'happened', 'me.', 'The', 'first', 'thing', 'struck', 'Oz', 'brutality', 'unflinching', 'scenes', 'violence', 'set', 'right', 'word', 'GO.', 'Trust', 'show', 'faint', 'hearted', 'timid.', 'This', 'show', 'pulls', 'punches', 'regards', 'drugs', 'sex', 'violence.', 'Its', 'hardcore', 'classic', 'use', 'word.', 'It', 'called', 'OZ', 'nickname', 'given', 'Oswald', 'Maximum', 'Security', 'State', 'Penitentary.', 'It', 'focuses', 'mainly', 'Emerald', 'City', 'experimental', 'section', 'prison', 'cells', 'glass', 'fronts', 'face', 'inwards', 'privacy', 'high', 'agenda.', 'Em', 'City', 'home', 'many..Aryans', 'Muslims', 'gangstas', 'Latinos', 'Christians', 'Italians', 'Irish', 'more....so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'shady', 'agreements', 'never', 'far', 'away.', 'I', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'goes', 'shows', 'dare.', 'Forget', 'pretty', 'pictures', 'painted', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance...OZ', 'mess', 'around.', 'The', 'first', 'episode', 'I', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'I', 'say', 'I', 'ready', 'I', 'watched', 'I', 'developed', 'taste', 'Oz', 'got', 'accustomed', 'high', 'levels', 'graphic', 'violence.', 'Not', 'violence', 'injustice', 'crooked', 'guards', \"who'll\", 'sold', 'nickel', 'inmates', \"who'll\", 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmates', 'turned', 'prison', 'bitches', 'due', 'lack', 'street', 'skills', 'prison', 'experience', 'Watching', 'Oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewing....thats', 'get', 'touch', 'darker', 'side.']\n",
            "['One', 'reviewer', 'mentioned', 'watching', '1', 'Oz', 'episode', 'hooked.', 'They', 'right', 'exactly', 'happened', 'me.', 'The', 'first', 'thing', 'struck', 'Oz', 'brutality', 'unflinching', 'scene', 'violence', 'set', 'right', 'word', 'GO.', 'Trust', 'show', 'faint', 'hearted', 'timid.', 'This', 'show', 'pull', 'punch', 'regard', 'drug', 'sex', 'violence.', 'Its', 'hardcore', 'classic', 'use', 'word.', 'It', 'called', 'OZ', 'nickname', 'given', 'Oswald', 'Maximum', 'Security', 'State', 'Penitentary.', 'It', 'focus', 'mainly', 'Emerald', 'City', 'experimental', 'section', 'prison', 'cell', 'glass', 'front', 'face', 'inwards', 'privacy', 'high', 'agenda.', 'Em', 'City', 'home', 'many..Aryans', 'Muslims', 'gangsta', 'Latinos', 'Christians', 'Italians', 'Irish', 'more....so', 'scuffle', 'death', 'stare', 'dodgy', 'dealing', 'shady', 'agreement', 'never', 'far', 'away.', 'I', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'go', 'show', 'dare.', 'Forget', 'pretty', 'picture', 'painted', 'mainstream', 'audience', 'forget', 'charm', 'forget', 'romance...OZ', 'mess', 'around.', 'The', 'first', 'episode', 'I', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'I', 'say', 'I', 'ready', 'I', 'watched', 'I', 'developed', 'taste', 'Oz', 'got', 'accustomed', 'high', 'level', 'graphic', 'violence.', 'Not', 'violence', 'injustice', 'crooked', 'guard', \"who'll\", 'sold', 'nickel', 'inmate', \"who'll\", 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmate', 'turned', 'prison', 'bitch', 'due', 'lack', 'street', 'skill', 'prison', 'experience', 'Watching', 'Oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewing....thats', 'get', 'touch', 'darker', 'side.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsxMKnTQjk32"
      },
      "source": [
        "# def remove_non_alpha(text):\n",
        "#     regex = re.compile(\"[^a-zA-Z1-9.']\")\n",
        "#     return [regex.sub('',w) for w in text] \n",
        "\n",
        "# df['review_only_alpha'] = df['review_lemmatize'].apply(remove_non_alpha)\n",
        "# print(df['review_lemmatize'][0])\n",
        "# print(df['review_only_alpha'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akm8QZEFLeob"
      },
      "source": [
        "# Experiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8z52pSvps9w"
      },
      "source": [
        "## Split \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bADID-zwpu7o",
        "outputId": "fa2588d9-2bc7-49a1-c454-dffbbcd2e80d"
      },
      "source": [
        "\n",
        "X = df['review_lemmatize']\n",
        "y = df['sentiment_binary']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22793    [The, horse, indeed, fine, animal., Picturesqu...\n",
              "33584    [And, I, do., Peter, Falk, created, role, live...\n",
              "27739    [Spanish, film, Golden, definitely, Silver, Ag...\n",
              "38163    [The, often, misunderstood, Zabriskie, Point, ...\n",
              "23100    [This, got, one, magnificent, thing, I've, eve...\n",
              "Name: review_lemmatize, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZjq_mOqhM2o"
      },
      "source": [
        "## Deeplearning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPYSyOEYMuNi"
      },
      "source": [
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(df['review_lemmatize'])\n",
        "\n",
        "vocab_size = len(tokenizer_obj.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eFYai4HmN8U",
        "outputId": "189716e0-b005-4393-eaa6-d8840b14c1e7"
      },
      "source": [
        "# tokens\n",
        "X_train_tokens = tokenizer_obj.texts_to_sequences(X_train)\n",
        "X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)\n",
        "\n",
        "# pad sequence \n",
        "length = max([len(s) for s in df['review_lemmatize']])\n",
        "X_train_seq = pad_sequences(X_train_tokens, maxlen=length, padding='post')\n",
        "X_test_seq = pad_sequences(X_test_tokens, maxlen=length, padding='post')\n",
        "\n",
        "print(X_train_tokens[0])\n",
        "print(X_train_seq[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 1292, 846, 438, 14340, 10230, 2141, 1269, 1292, 1540, 29, 42, 12167, 787, 1657, 2, 787, 242, 62116, 2, 438, 787, 708, 13865, 882, 1292, 122905, 534, 1657, 1609, 18006, 28330, 6462, 14113, 1241, 534, 4215, 2733, 18183, 722, 4822, 2, 333, 11961, 143, 7247, 1041, 47, 136, 45, 19, 1283, 335, 18802, 1292, 9, 3910, 22, 2, 19, 438, 1292, 174, 335, 18802, 1283, 9, 205, 3, 607, 54, 15733, 5, 54, 787, 414, 5708, 14, 438, 876, 2742, 2035, 688, 700, 377, 36, 9]\n",
            "[   2 1292  846 ...    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQoXI6Etynwy"
      },
      "source": [
        "embed_dim = 100\n",
        "window = 5 \n",
        "workers = 4\n",
        "model = gensim.models.Word2Vec(sentences=df['review_lemmatize'], size=embed_dim, window=window, workers=workers,min_count=1)\n",
        "vocab_model = model.wv.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1endvjDJ0Fsz",
        "outputId": "1e7b5c51-ad63-4ca3-e429-e86434f85917"
      },
      "source": [
        "model.wv.most_similar('good')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('decent', 0.7993804216384888),\n",
              " ('great', 0.7634856700897217),\n",
              " ('nice', 0.7180683016777039),\n",
              " ('good.', 0.7103123664855957),\n",
              " ('fine', 0.6992824077606201),\n",
              " ('bad', 0.6693556904792786),\n",
              " ('excellent', 0.6501373648643494),\n",
              " ('okay', 0.6463180780410767),\n",
              " ('alright', 0.6403177976608276),\n",
              " ('cool', 0.614944577217102)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hGl-Q382fVw"
      },
      "source": [
        "embed_idx = {}\n",
        "i = 0\n",
        "for word in vocab_model : \n",
        "  embed_idx[word] = model.wv.vectors[i]\n",
        "  i+=1\n",
        "\n",
        "# embed_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwQ89UCu1CTG"
      },
      "source": [
        "embed_matrix = np.zeros((vocab_size+1,embed_dim))\n",
        "\n",
        "for word, idx in tokenizer_obj.word_index.items() : \n",
        "  if i > (vocab_size+1) :\n",
        "    continue \n",
        "  if embed_idx[word] is not None : \n",
        "    embed_matrix[i] = embed_idx[word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhyaUVot-8tL"
      },
      "source": [
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Embedding, LSTM, GRU\n",
        "from keras.layers.embeddings import Embedding \n",
        "from keras.initializers import Constant \n",
        "\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size+1, embed_dim, embeddings_initializer=Constant(embed_matrix), input_length=length, trainable=False)\n",
        "model.add(embedding_layer) \n",
        "model.add(LSTM(units=32, dropout=0.2))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ4UfZap_lNI",
        "outputId": "5958b891-29f4-40e5-ed76-4dbc4cd7e052"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1519, 100)         16737200  \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                17024     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,754,257\n",
            "Trainable params: 17,057\n",
            "Non-trainable params: 16,737,200\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euzUotfA_nkA"
      },
      "source": [
        "# X_train_seq\n",
        "\n",
        "model.fit(X_train_seq, y_train, batch_size=100, epochs=25, validation_data=(X_test_seq,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6pdd7U-Gvta"
      },
      "source": [
        "## Naivebayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlQoNpluG0qp",
        "outputId": "62a15aef-b31c-4c96-9ece-3d4dfca39ff9"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn import metrics \n",
        "\n",
        "#tfidf\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "temp = df['review'].apply(lambda x: remove_tags(x))\n",
        "temp_lemmatize = temp.apply(lambda x : WordNetLemmatizer().lemmatize(x,'v'))\n",
        "# print(temp)\n",
        "X = tfidf.fit_transform(temp_lemmatize)\n",
        "vocabulary = tfidf.get_feature_names() \n",
        "# X1 = pd.DataFrame(data=X.toarray(), columns= vocabulary)\n",
        "# print(X[0].dtypes) \n",
        "# print(X1)\n",
        "y = df['sentiment_binary']\n",
        "\n",
        "# print(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "mnb = MultinomialNB() \n",
        "mnb.fit(X_train,y_train)\n",
        "\n",
        "predicted_mnb = mnb.predict(X_test)\n",
        "accuracy_score = metrics.accuracy_score(predicted_mnb, y_test)\n",
        "accuracy_score\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.867"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzmoWBhgIb4C",
        "outputId": "1fcd10a6-f53b-478b-f7e7-bc10548cc52a"
      },
      "source": [
        "# tfidf\n",
        "cv = CountVectorizer()\n",
        "# X = cv.fit_transform(df['review'])\n",
        "X = cv.fit_transform(temp_lemmatize)\n",
        "y = df['sentiment_binary']\n",
        "\n",
        "# print(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "mnb = MultinomialNB() \n",
        "mnb.fit(X_train,y_train)\n",
        "\n",
        "predicted_mnb = mnb.predict(X_test)\n",
        "accuracy_score = metrics.accuracy_score(predicted_mnb, y_test)\n",
        "accuracy_score\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.842"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbkPWesSJwz_"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjWIk0RdKY84",
        "outputId": "da473d63-ab96-4de2-db7d-64dfa733f554"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "X = cv.fit_transform(temp_lemmatize)\n",
        "y = df['sentiment_binary']\n",
        "\n",
        "# print(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "LR = LogisticRegression()\n",
        "LR.fit(X_train,y_train)\n",
        "\n",
        "predicted_lr = LR.predict(X_test)\n",
        "accuracy_score = metrics.accuracy_score(predicted_lr, y_test)\n",
        "accuracy_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8815"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OCOOnaOkukM",
        "outputId": "c90e51a9-c708-4d01-cddf-6b667538f3a3"
      },
      "source": [
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "X = tfidf.fit_transform(temp)\n",
        "y = df['sentiment_binary']\n",
        "\n",
        "# print(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "LR = LogisticRegression()\n",
        "LR.fit(X_train,y_train)\n",
        "\n",
        "predicted_lr = LR.predict(X_test)\n",
        "accuracy_score = metrics.accuracy_score(predicted_lr, y_test)\n",
        "accuracy_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8982"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErCNBfS8l3o_",
        "outputId": "304aafde-1bc3-4e0c-daa4-f868bff14436"
      },
      "source": [
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "X = tfidf.fit_transform(temp_lemmatize)\n",
        "y = df['sentiment_binary']\n",
        "\n",
        "# print(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "LR = LogisticRegression()\n",
        "LR.fit(X_train,y_train)\n",
        "\n",
        "predicted_lr = LR.predict(X_test)\n",
        "accuracy_score = metrics.accuracy_score(predicted_lr, y_test)\n",
        "accuracy_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8938"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}